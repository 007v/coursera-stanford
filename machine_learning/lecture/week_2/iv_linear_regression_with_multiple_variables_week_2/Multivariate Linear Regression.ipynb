{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Multiple Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic1.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features) 3:05*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic2.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features) 3:24*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\large h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For convenience of notation, we define $\\large x_{0} = 1$.\n",
    "- Previously, we had n features starting from $\\large x_{1}, x_{2} ... x_{n}$. Now after we define an additional zero feature, now feature vector x becomes n + 1 dimensional vector that is zero index.\n",
    "- For the parameter vector $\\large \\theta$, again, this is another zero index vector staring from $\\large \\theta_{0}$, so this is another n + 1 dimensional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic3.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/6Nj1q/multiple-features) 8:15*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\large \\theta^{T}$ is a (1 by n) matrix\n",
    "- x is a (n by 1) matrix\n",
    "- The result of $\\large \\theta^{T}x$ is a (1 by 1) constant value\n",
    "\n",
    "The form of the hypothesis is just the inner product between our parameter vector theta and our theta vector X.\n",
    "\n",
    "The term multivariable is just maybe a fancy term for saying we have multiple features, or multivariables with which to try to predict the value Y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Gradient Descent for Multiple Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic4.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Z9DKX/gradient-descent-for-multiple-variables) 1:20*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For this quiz, we have:\n",
    "$$\\large h_{\\theta}(x) = \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} = \\theta^{T}x$$\n",
    "\n",
    "    - Answer 1:  $\\large h_{\\theta}(x^{(i)}) = \\theta^{T}x^{(i)} $\n",
    "    - Answer 2:\n",
    "    $\\large h_{\\theta}(x^{(i)})= \\theta_{0}x_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + ... + \\theta_{n}x_{n} = (\\sum_{j=0}^n \\theta_{j}x_{j}^{(i)}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic5.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Z9DKX/gradient-descent-for-multiple-variables) 1:47*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new and old algorithms are both similar to each other. If you consider a case where we have two features, then we have three update rules for the parameters $\\theta_{0}, \\theta_{1}, \\theta_{2}$.\n",
    "\n",
    "- If you look at the update rule for $\\theta_{0}$, what you find is that this update rule here is the same as the update rule that we had previously for the case of n = 1. Because in our notational convention we had $x_{0}^{(i)} = 1$.\n",
    "- Same thing if you compare the new $\\theta_{1}$ equation with the old one, you find them similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic6.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/Z9DKX/gradient-descent-for-multiple-variables) 4:57*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Gradient Descent in Practice: Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Idea: Make sure features are on a similar scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, if you have 2 feafures:\n",
    "- x1: size of the house\n",
    "- x2: number of bedrooms\n",
    "\n",
    "If those feature are not on a similar scale, when you plot the contours of the cost function $\\large J_{\\theta}$, you will have a skewed elliptical shape. And if you run gradient descents on this cost function, your gradients may end up taking a long time and can oscillate back and forth, before it can finally find its way to the global minimum.\n",
    "\n",
    "When you divide x1 by 2000, and x2 by 5, you have a less skewed contour cost function. Also when you run the gradient descent, you can find a much more direct path to the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/lec4_pic7.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling) 3:00*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure features are on a similar scale.**\n",
    "\n",
    "- Get every feature into approximately a $-1 <= x_{i} <= 1$ range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Mean Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $x_{i}$: feature\n",
    "- $\\mu_{i}$: mean (avarage) value of x (feature)\n",
    "- $s_{i}$: range of the feature (max - min) or you can set $s_{i}$ as the standard deviation of the feature.\n",
    "\n",
    "$$\\large x_{i} = \\dfrac{x_{i} - \\mu_{i}}{s_{i}}$$\n",
    "\n",
    "For example,\n",
    "- $x_{1}$: size of the house. We have the average size of a house is 1000. Then we set feature $x_{1} = \\dfrac{size - 1000}{2000}$\n",
    "- $x_{2}$: number of bedrooms. We have the average size of a house is 2. Then $x_{2} = \\dfrac{bedrooms - 2}{5}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic8.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling) 8:15*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic9.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/xx3Da/gradient-descent-in-practice-i-feature-scaling) 8:35*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Gradient Descent in Practice: Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Making sure gradient descent is working correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If gradient descent is working properly, then the cost function $J_{\\theta}$ should decrease after every iteration.\n",
    "\n",
    "One useful thing in the plot below is that it looks like by the time you've gotten out to maybe 300 iterations, between 300 and 400 iterations, $J_{\\theta}$ hasn't gone down much more. By the time you get to 400 iterations, it looks like this curve has flattened out here. This means that at 400 iterations, gradient descent has more or less <u>**converged**</u> because your cost function isn't going down much more.\n",
    "\n",
    "So, looking at this figure can also help you judge whether or not gradient descent has converged. Also, the number of iterations\n",
    "the gradient descent takes to converge for a physical application can vary a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic10.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate) 4:22*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see $J_{\\theta}$ is actually increasing, then that gives you a clear sign that gradient descent is not working. It usually means that you should decrease the learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic11.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate) 6:50*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic12.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate) 6:51*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I can do when I try to run gradient descent is I would try a range of values, for example: 0.001, 0.003, 0.01, ... until I find a good learning rate for the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/lec4_pic13.png\">\n",
    "\n",
    "*Screenshot taken from [Coursera](https://www.coursera.org/learn/machine-learning/lecture/3iawu/gradient-descent-in-practice-ii-learning-rate) 8:35*\n",
    "\n",
    "<!--TEASER_END-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Features and polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
